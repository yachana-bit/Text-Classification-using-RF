{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6Et_2Kd2yEG"
      },
      "source": [
        "#Importing the required libraries\n",
        "from gensim.models import Word2Vec, word2vec\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYeuTk-u3F_X",
        "outputId": "28909b36-7497-45bd-e3ff-e3c34fbf9c13"
      },
      "source": [
        "# Load the punkt tokenizer used for splitting reviews into sentences\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "KsvLUJTM6WIL",
        "outputId": "afa037cb-d2fb-43bd-a720-044c855b1cc5"
      },
      "source": [
        "main_data = pd.read_csv('dataset.csv')\n",
        "main_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>China had role in Yukos split-up\\n \\n China le...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Oil rebounds from weather effect\\n \\n Oil pric...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Indonesia 'declines debt freeze'\\n \\n Indonesi...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>$1m payoff for former Shell boss\\n \\n Shell is...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US bank in $515m SEC settlement\\n \\n Five Bank...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                news      type\n",
              "0  China had role in Yukos split-up\\n \\n China le...  business\n",
              "1  Oil rebounds from weather effect\\n \\n Oil pric...  business\n",
              "2  Indonesia 'declines debt freeze'\\n \\n Indonesi...  business\n",
              "3  $1m payoff for former Shell boss\\n \\n Shell is...  business\n",
              "4  US bank in $515m SEC settlement\\n \\n Five Bank...  business"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMaANJefHanY",
        "outputId": "cc869d10-11f8-4f8e-e58f-2c7281754d0e"
      },
      "source": [
        "print(main_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2225, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-5uiGbU3Msf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "047ebe65-1f00-4189-aa74-591ce9d61be4"
      },
      "source": [
        "num_data, meta_data = pd.factorize(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-eb1caa050a3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgyo-81dEmzp",
        "outputId": "98a82004-b6a8-481d-efc6-5383e70ac2e2"
      },
      "source": [
        "print(num_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 4 4 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d-49Moz3Ud_"
      },
      "source": [
        "#Convert a news to a list of words\n",
        "def news_to_wordlist(news, remove_stopwords=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    news = news from the dataset\n",
        "    remove_stopwords = boolean value\n",
        "\n",
        "    Output:\n",
        "    words = lists of words in a aingle news\n",
        "    \"\"\"\n",
        "    # remove non-letters\n",
        "    news_text = re.sub(\"[^a-zA-Z]\",\" \", news)\n",
        "\n",
        "    # convert to lower case and split at whitespace\n",
        "    words = news_text.lower().split()\n",
        "\n",
        "    # remove stop words (false by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEPO6PuX4GZc"
      },
      "source": [
        "#Split news into list of sentences where each sentence is a list of words.\n",
        "def news_to_sentences(news, tokenizer, remove_stopwords=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    news = news to be tokenized\n",
        "    tokenizer = the nltk tokenizer\n",
        "    remove_stopwords = boolean value\n",
        "\n",
        "    Output:\n",
        "    sentences = list of sentences where each sentence is also a list of words\n",
        "    \"\"\"\n",
        "    # use the NLTK tokenizer to split the paragraph into sentences\n",
        "    raw_sentences = tokenizer.tokenize(news.strip())\n",
        "\n",
        "    # each sentence is furthermore split into words\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        # If a sentence is empty, skip it\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(news_to_wordlist(raw_sentence, remove_stopwords))\n",
        "\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMxbmH484bsd"
      },
      "source": [
        "train_sentences = []  # Initialize an empty list of sentences\n",
        "for news in main_data['news']:\n",
        "    # Converting each news into sentences\n",
        "    train_sentences += news_to_sentences(news, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_lPXsBc6fEm",
        "outputId": "84bf3018-47d2-4d9b-b7af-e58b3341238d"
      },
      "source": [
        "train_sentences[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['china',\n",
              " 'had',\n",
              " 'role',\n",
              " 'in',\n",
              " 'yukos',\n",
              " 'split',\n",
              " 'up',\n",
              " 'china',\n",
              " 'lent',\n",
              " 'russia',\n",
              " 'bn',\n",
              " 'bn',\n",
              " 'to',\n",
              " 'help',\n",
              " 'the',\n",
              " 'russian',\n",
              " 'government',\n",
              " 'renationalise',\n",
              " 'the',\n",
              " 'key',\n",
              " 'yuganskneftegas',\n",
              " 'unit',\n",
              " 'of',\n",
              " 'oil',\n",
              " 'group',\n",
              " 'yukos',\n",
              " 'it',\n",
              " 'has',\n",
              " 'been',\n",
              " 'revealed']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stOoF8I66qaA"
      },
      "source": [
        "#Set values for various word2vec parameters\n",
        "num_features = 300    # Word vector dimensionality\n",
        "min_word_count = 40   # Minimum word count\n",
        "num_workers = 3       # Number of threads to run in parallel\n",
        "context = 10          # Context window size\n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "# Initialize and train the model (this will take some time)\n",
        "model = word2vec.Word2Vec(train_sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
        "\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "# Saving the model for later use.\n",
        "model.save('model_name')\n",
        "\n",
        "# Loading the model\n",
        "model = Word2Vec.load('model_name')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc5DhYL1A2Zq"
      },
      "source": [
        "#Average the word vectors for a set of words\n",
        "def make_feature_vec(words, model, num_features):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    words = words in a news\n",
        "    model = the word2Vec model\n",
        "    num_features = number of features for the model\n",
        "\n",
        "    Output:\n",
        "    feature_vec = returns the average word vector from the feature vec of a set of words\n",
        "    \"\"\"\n",
        "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
        "    nwords = 0\n",
        "    index2word_set = set(model.wv.index2word)  # words known to the model\n",
        "\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1\n",
        "            feature_vec = np.add(feature_vec,model[word])\n",
        "\n",
        "    feature_vec = np.divide(feature_vec, nwords)\n",
        "    return feature_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jfx74n4BTwD"
      },
      "source": [
        "#Calculate average feature vectors for all reviews\n",
        "def get_avg_feature_vecs(news, model, num_features):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    news = news\n",
        "    model = the word2Vec model\n",
        "    num_features = number of features in the feature vector\n",
        "\n",
        "    Output:\n",
        "    news_feature_vecs = Feature vector of each news by averaging feature vectors of each words in a news\n",
        "    \"\"\"\n",
        "    counter = 0\n",
        "    news_feature_vecs = np.zeros((len(news),num_features), dtype='float32')  # pre-initialize (for speed)\n",
        "\n",
        "    for new in news:\n",
        "        news_feature_vecs[counter] = make_feature_vec(new, model, num_features)\n",
        "        counter = counter + 1\n",
        "    return news_feature_vecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEmZSTxACdqa",
        "outputId": "b23f09f7-c745-4335-fae5-44fde92c525e"
      },
      "source": [
        "clean_train_news = []\n",
        "for new in main_data['news']:\n",
        "    clean_train_news.append(news_to_wordlist(new, remove_stopwords=True))\n",
        "trainDataVecs = get_avg_feature_vecs(clean_train_news, model, num_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-xr1h1rECQA",
        "outputId": "1dbe5171-e2a3-4f98-82b6-fce19a9e5268"
      },
      "source": [
        "print(trainDataVecs[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-2.0841040e-02 -3.5884351e-02  4.1210413e-02 ... -2.5158955e-02\n",
            "   7.8683123e-03  4.9836688e-02]\n",
            " [-1.7440600e-02 -3.0171243e-02  2.6495187e-02 ... -2.5629971e-02\n",
            "   6.3768323e-03  4.8588254e-02]\n",
            " [-5.0745177e-04 -3.8599502e-02  2.9329101e-02 ... -2.2326024e-02\n",
            "   2.2007537e-03  3.6792152e-02]\n",
            " [-6.4780205e-03 -3.4437228e-02  1.7679147e-02 ... -1.2783796e-02\n",
            "  -5.8734096e-03  3.2464299e-02]\n",
            " [-1.9787379e-02 -5.0354157e-02  2.9083764e-02 ... -2.5976785e-02\n",
            "   2.3560506e-06  4.1006099e-02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vydVanSG2ha",
        "outputId": "54ac36ed-f4f9-4286-ef04-f5b5a6776f54"
      },
      "source": [
        "print(len(trainDataVecs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBzEGUCELxnv"
      },
      "source": [
        "#Random Forest Classifier\n",
        "from collections import Counter\n",
        "\n",
        "#function to calculate entropy\n",
        "def entropy(y):\n",
        "    \"\"\"\"\n",
        "    Arguments:\n",
        "    y = the labels\n",
        "\n",
        "    Output:\n",
        "    entropy of the input\n",
        "    \"\"\"\n",
        "    hist = np.bincount(y)\n",
        "    ps = hist / len(y)\n",
        "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
        "\n",
        "#Class for node of decision trees\n",
        "class Node:\n",
        "    #Initializing the class\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    #Checking whether the node is a leaf node or not\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "#Class for decision tree\n",
        "class DecisionTree:\n",
        "\n",
        "    #Initializing the class\n",
        "    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.n_feats = n_feats\n",
        "        self.root = None\n",
        "\n",
        "    #Method to fit the decision tree\n",
        "    def fit(self, X, y):\n",
        "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    #Method to predict\n",
        "    def predict(self, X):\n",
        "        #traversing the tree from the root\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    #Method to grow trees\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # stopping criteria\n",
        "        if (depth >= self.max_depth\n",
        "                or n_labels == 1\n",
        "                or n_samples < self.min_samples_split):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        #randomly select features\n",
        "        feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n",
        "\n",
        "        # greedily select the best split according to information gain\n",
        "        best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
        "\n",
        "        # grow the children that result from the split\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "        return Node(best_feat, best_thresh, left, right)\n",
        "\n",
        "    #Selecting the best split point\n",
        "    def _best_criteria(self, X, y, feat_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_thresh = None, None\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            thresholds = np.unique(X_column)\n",
        "            for threshold in thresholds:\n",
        "                gain = self._information_gain(y, X_column, threshold)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_thresh = threshold\n",
        "\n",
        "        return split_idx, split_thresh\n",
        "\n",
        "    #To calculate the information gain\n",
        "    def _information_gain(self, y, X_column, split_thresh):\n",
        "        #parent loss\n",
        "        parent_entropy = entropy(y)\n",
        "\n",
        "        # generate split\n",
        "        left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
        "\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        # compute the weighted avg. of the loss for the children\n",
        "        n = len(y)\n",
        "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n",
        "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
        "\n",
        "        # information gain is difference in loss before vs. after split\n",
        "        ig = parent_entropy - child_entropy\n",
        "        return ig\n",
        "\n",
        "    #Splitting the data on the basis of split threshold\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    #Traversing the tree\n",
        "    def _traverse_tree(self, x, node):\n",
        "        #returns the value if the node is a leaf\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "        #Traverse to the left if the feature is lesser than the threshold\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "\n",
        "    # Calculating the most common label\n",
        "    def _most_common_label(self, y):\n",
        "        counter = Counter(y)\n",
        "        most_common = counter.most_common(1)[0][0]\n",
        "        return most_common"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMdvLD-eEYbT"
      },
      "source": [
        "# Generating the random samples\n",
        "def bootstrap_sample(X, y):\n",
        "    n_samples = X.shape[0]\n",
        "    idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
        "    return X[idxs], y[idxs]\n",
        "\n",
        "def most_common_label(y):\n",
        "    counter = Counter(y)\n",
        "    most_common = counter.most_common(1)[0][0]\n",
        "    return most_common\n",
        "\n",
        "#Random Forest Class\n",
        "class RandomForest:\n",
        "    #Initilizing the class\n",
        "    def __init__(self, n_trees=10, min_samples_split=2,\n",
        "                 max_depth=100, n_feats=None):\n",
        "        self.n_trees = n_trees\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.n_feats = n_feats\n",
        "        self.trees = []\n",
        "\n",
        "    # Method to fit the model\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        #Generates n number of trees\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTree(min_samples_split=self.min_samples_split,\n",
        "                max_depth=self.max_depth, n_feats=self.n_feats)\n",
        "            X_samp, y_samp = bootstrap_sample(X, y)\n",
        "            tree.fit(X_samp, y_samp)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    # Method to predict result from the model built\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
        "        y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n",
        "        return np.array(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us14vt31Oxmf"
      },
      "source": [
        "#Splitting the dataset into training and testing\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainDataVecs,num_data, test_size=0.2, random_state=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqTAntV8EgNZ"
      },
      "source": [
        "#Calling the randomforest classifier\n",
        "random_clf = RandomForest(n_trees=3)\n",
        "random_clf.fit(X_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1owQ2JEdQli-"
      },
      "source": [
        "#To calculate the accuracy of the model\n",
        "def accuracy(y_true, y_pred):\n",
        "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chDZLLJRQ7-q",
        "outputId": "411737fe-95bd-43c1-ed58-77a95f8642e1"
      },
      "source": [
        "#Calling the predict method on testing dataset\n",
        "Y_pred = random_clf.predict(X_test)\n",
        "\n",
        "#Finding the accuracy\n",
        "acc = accuracy(Y_test, Y_pred)\n",
        "\n",
        "print (\"Accuracy:\", acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8808988764044944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYA_JxZTTXNY"
      },
      "source": [
        "class LogisticRegression(object):\n",
        "\n",
        "    def __init__(Logreg, alpha=0.01, n_iteration=100):  #This function intializes the alpha value and iteration\n",
        "        Logreg.alpha = alpha                            #value in the object\n",
        "        Logreg.n_iter = n_iteration\n",
        "\n",
        "    def _sigmoid_function(Logreg, x): #This function is resonsible for calculating the sigmoid value with given parameter\n",
        "        value = 1 / (1 + np.exp(-x))\n",
        "        return value\n",
        "    def _cost_function(Logreg,h,theta, y): # The fuctions calculates the cost value\n",
        "        m = len(y)\n",
        "        cost = (1 / m) * (np.sum(-y.T.dot(np.log(h)) - (1 - y).T.dot(np.log(1 - h))))\n",
        "        return cost\n",
        "\n",
        "    def _gradient_descent(Logreg,X,h,theta,y,m): # This function calculates the theta value by gradient descent\n",
        "        gradient_value = np.dot(X.T, (h - y)) / m\n",
        "        theta -= Logreg.alpha * gradient_value\n",
        "        return theta\n",
        "\n",
        "    def fit(Logreg, X, y): #This function primarily calculates the optimal theta value using which we predict the future data\n",
        "        print(\"Fitting the given dataset..\")\n",
        "        Logreg.theta = []\n",
        "        Logreg.cost = []\n",
        "        X = np.insert(X, 0, 1, axis=1)\n",
        "        m = len(y)\n",
        "        for i in np.unique(y):\n",
        "            #print('Descending the gradient for label type ' + str(i) + 'vs Rest')\n",
        "            y_onevsall = np.where(y == i, 1, 0)\n",
        "            theta = np.zeros(X.shape[1])\n",
        "            cost = []\n",
        "            for _ in range(Logreg.n_iter):\n",
        "                z = X.dot(theta)\n",
        "                h = Logreg._sigmoid_function(z)\n",
        "                theta = Logreg._gradient_descent(X,h,theta,y_onevsall,m)\n",
        "                cost.append(Logreg._cost_function(h,theta,y_onevsall))\n",
        "            Logreg.theta.append((theta, i))\n",
        "            Logreg.cost.append((cost,i))\n",
        "        return Logreg\n",
        "\n",
        "    def predict(Logreg, X): # this function calls the max predict function to classify the individul feauter\n",
        "        X = np.insert(X, 0, 1, axis=1)\n",
        "        X_predicted = [max((Logreg._sigmoid_function(i.dot(theta)), c) for theta, c in Logreg.theta)[1] for i in X ]\n",
        "\n",
        "        return X_predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQheghhKUA9W",
        "outputId": "983e33e3-e69a-4f30-bb83-270964082061"
      },
      "source": [
        "#Calling the logistic regression method on the training dataset\n",
        "logi = LogisticRegression(n_iteration=30000).fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting the given dataset..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ2cAPxIUPbP",
        "outputId": "2b744a7d-a190-4b60-cd35-7d6833cf2d50"
      },
      "source": [
        "# Prediction on test set\n",
        "prediction = logi.predict(X_test)\n",
        "acc = accuracy(Y_test, prediction)\n",
        "\n",
        "print (\"Accuracy:\", acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.802247191011236\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}